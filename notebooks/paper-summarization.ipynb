{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..') + '/src')\n",
    "\n",
    "import base64\n",
    "\n",
    "from anthropic_api import AnthropicAPI\n",
    "from summarize_pdfs import summarize_pdfs_in_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../data/pdfs/2024.findings-emnlp.31.pdf\"\n",
    "prompt_path = \"../data/prompts/paper_summarization_v2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AnthropicAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = api_client.summarize_pdf(pdf_path, prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_018cHX2pxkeRJENWCifLvsgb',\n",
       " 'content': [{'text': \"Here's a focused summary addressing the key points:\\n\\nThe paper proposes CLINICR, a novel prompting methodology for medical question answering using Large Language Models (LLMs). The authors modified the MedQA-USMLE dataset to create MEDQA-OPEN, which contains open-ended medical questions without multiple-choice options, making it more representative of real clinical scenarios. They also introduced CLINICIANCASES, a dataset of 25 real-life complex medical cases from practicing professionals. The research utilized different prompting strategies: CLINICR (using incremental clinical reasoning), ELIMINATIVE (option elimination-based), and hybrid approaches combining both methods with a verifier model. The authors trained verifier models using both Llama-2-7B-chat and Llama-2-70B-chat as base models, implementing Low-Ranked Adaptation (LoRA) for fine-tuning.\\n\\nFor evaluation, eight final-year medical students assessed the responses generated by different approaches on a 3-point Likert scale (Agree, Neutral, Disagree). The evaluation covered 500 questions from MEDQA-OPEN and all 25 questions from CLINICIANCASES. Results showed that CLINICR significantly outperformed ELIMINATIVE on the 7B model (83% vs 56% agreement) and performed slightly better on the 70B model (87% vs 84% agreement). The forward-backward approach using CLINICR with verifier achieved the highest performance, reaching 87% agreement on both models. The code and prompts used in the experiments are available at https://github.com/ColdSeal/ClinicR. The paper demonstrates that incremental reasoning-based prompting performs better than elimination-based approaches for open-ended medical questions.\",\n",
       "   'type': 'text'}],\n",
       " 'model': 'claude-3-5-sonnet-20241022',\n",
       " 'role': 'assistant',\n",
       " 'stop_reason': 'end_turn',\n",
       " 'stop_sequence': None,\n",
       " 'type': 'message',\n",
       " 'usage': {'cache_creation_input_tokens': 0,\n",
       "  'cache_read_input_tokens': 80685,\n",
       "  'input_tokens': 332,\n",
       "  'output_tokens': 396}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_dir = \"../data/pdfs\"\n",
    "prompt_path = \"../data/prompts/paper_summarization_v2.txt\"\n",
    "output_path = \"../data/emnlp2024_medical_qa.json\"\n",
    "\n",
    "# summarize_pdfs_in_directory(pdfs_dir, prompt_path, output_path, delay=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alchemy-env",
   "language": "python",
   "name": "alchemy-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
